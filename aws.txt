
AWS

Aws regions
        aws availability zones ( 3 min, 6 max, usually 3)


EC2 -> regional service
IAM -> global service

IAM -> Identity and access management
      users
      groups
      roles
policies written in JSON


IAM federation -> for big enterprise -> SAML standard

One IAM user per physical person
One IAM role per application


EC2

capability of
1) renting virtual machines (EC2)
2) storing data on virtual drives (EBS)
3) distributing load across machines( ELB)
4) Scaling the services using auto-scaling group(ASG)

Steps
AMI -> amazon machine image -> instance type t2.micro -> configuration -> storage -> tags -> security group

AMI -> Built for specific AWS Region

pem file permission -> chmod 400

security group -> inbound and outbound -> acting as firewall
                can be to multiple instances
                locked down to a region /VPC combination

elastic IP -> its a public ipv4 own as long as we don't delete it
                  max 5 elastic in a account

EC2 User data (runs with root user ) -> bootstrapping our instances -> launching commands when machine starts
              copy your shell script to user data field

EC2 Launch types -> on demand instances -  short workload , predictable pricing
                    reserved - (minimum 1 year or 3 year )
                            reserved instances - 75% discount
                            convertible reserved instances  - 54% discount
                            scheduled reserved instances  -
                    Spot instances - 90% discount ( can loose at any point of time ) - use if workloads are resilient to failure
                    dedicated instances -> no other customers can share this hardware. only for u , no control in placement
                    dedicated hosts -> Physical dedicated ec2 server , full control of ec2 instance placement , 3 year period reservation

ENI -> ELastic network interface -> Bounded to specific Availability zone
        logical component in VPC - represent a virtual network card
        primary ipv4, one or more secondary ipv4
        one elastic ipv4
        one public ipv4
        one or more security groups
        a mac address


EC2 Pricing -> Price per hour varies on
            region
            Instance type
            on-demand vs spot vs dedicated vs reserved
            linux vs windows vs private OS
Billed by second with min. of 60 seconds. If instanced stopped no payment
others factors are storage, load balancing etc etc

EC2 types based on -> RAM, CPU , I/O , Network , GPU
          M instances are balanced in everything No GPU
          T2, T3 are burstable instances (ok CPU performance). uses burst credits(CPU credits) -> all credits used-> CPU bad

For exam->
• Know how to SSH into EC2 (and change .pem file permissions)
• Know how to properly use security groups
• Know the fundamental differences between private vs public vs elastic IP
• Know how to use User Data to customize your instance at boot time
• Know that you can build custom AMI to enhance your OS
• EC2 instances are billed by the second and can be easily created and thrown away, welcome to the cloud!

=========================================================================================================

AWS - ELB - ASG

scalability -> horizontal and vertical
          horizontal - auto scaling, load balancer

High availability -> running in atleast 2 datacenters( availability zones)
          auto scaling group multiple AZ
          load balancer multi AZ

Load balancer
      - Expose single point of access (DNS) to your app
      - handle failures of downstream instances and do health checks

ELB -> Elastic load balancing ( EC2 load balancer)
      forward traffic to multiple servers( EC2 instances) downstream

      Health checks (/health is common) done on a port and route -> 200 OK check

      trobuleshooting
      4xx errors are client induced
      5xx errors are Application induced
      503 means at capacity or no registered target

      Monitoring
      ELB acces logs
      Cloudwatch metrics

Types -> Classic load balancer -> v1 -> http, https, tcp
         application load balancer -> v2 -> http, https, , websocket
         Network load balancer -> v2 -> TCP, TLS (secure TCP), UDP

         can be internal or external load balancer

Load balancer Security group
Application security group -> allow traffic only from load balancer

CLB -> Classic load balancer
      -> TCP ( layer 4), HTTP/HTTPS (layer 7)
      -> health checks are tcp or http based
      -> exposes static DNS(URL) fixed hostname xxx.region.elb.amazonaws.com

ALB -> Application load balancer
      - Layer 7 (http)
      - load balancing to multiple http apps across machines (target groups)
      - load balancing to multiple apps on same machine (containers)
      - support http/2 and websocket
      - supports redirects ( from http to https )
      - Routing
            - based on path in URL
            - based on hostnamme in URl
            - based on query strings and headers
      - Best fit for micro-services and container based application
      - Port mapping feature for redirection to Dynamic port
      - exposes static DNS(URL) fixed hostname xxx.region.elb.amazonaws.com
      - the application servers don't see the ip of client directly ( true ip in X-forwarded-for, port - x-forwarded-port, proto- x-frowarded-proto)
      - Create a security group and attach to the ALB security group
      Target group
          - EC2 instances ( managed by auto scaling group ) - http
          - ECS tasks ( manaeged by ECS ) - HTTP
          - lambda functions - http request is translated into json event
          - IP addresses - must be private IP
          - health checks are at target group level



NLB -> Network load balancer
        - Layer 4 - TCP and UDP
        - handles millions of requests and less latency ~ 100ms ( vs 400ms ALB )
        - Has one static IP per AZ and supports assigning Elastic IP


        create tcp RULE in security target group to accept traffic


Stickiness -> Same client always redirected to same instance behind LB ( works for CLB and ALB)
           -> Cookie used for stickiness has an expiration date you control
           -> may bring imbalance to loads over instances

Cross-Zone -> Each LB will distribute evenly across all Registered instances in all AZ
            -> If not works for only its AZ only

            CLB -> Disabled by default -> No charges for inter AZ data if enabled
            ALB -> Always on ( cant disable) -> No charges for inter AZ data
            NLB -> Disabled by default -> pay charges for inter AZ data if enabled

SSL/TLS -> traffic between client and LB is encrypted in transit
          - Public SSL certificates are issued by CA
          - have expiration date

ACM - aws certificate manager ,LB uses X.509 certificate

SNI -> Server Name indication
     - Solves the problem of  loading multiple SSL certificates onto one web server (to serve multiple websites)
     -> ONLY works for ALB, NLB , Cloudfront
     -> Doest not work for CLB ( older generation )

SSL certificates ->
            CLB
                  - Support only one SSL certificate
                  - must use multiple CLB for multiple hostname with multiple SSL certificates
            ALB
                  - supports multiple listeners with multiple SSL certificates
                  - uses SNI to make it work
            NLB
                  - supports multiple listeners with multiple SSL certificates
                  - uses SNI to make it work

Connection Draining - time to complete 'in-flight' requests while instance if de-registering or unhealthy
      Naming
            CLB -> Connection Draining
            target group - De-registration delay ( for ALB, NLB)
      - Stops sending new requests to instance which is de-registering
      - default is 300 seconds - 1 to 3600 seconds - can be disabled also (0 value)

ASG -> Auto Scaling Groups - min size - actual/desired size - max size
      - Scale out - ( Add instances)
      - scale In - ( remove EC2 instances)
      - min - max number of ec2
      - automatically register instances to load balancer
      - Its free
      - resiliency inbuilt
      - IAM roles attached to ASG attached to instances
      Attributes
          - launch Configuration / Launch templates
              - AMI + instance type
              - EC2 user data
              - EBS volumes
              - security groups
              - ssh key-pair
          - min/max/ intial capcity
          - network + subnet info
          - LB info
          - scaling policies

Auto Scaling Alarms -> cloudwatch alarms
Auto Scaling rules
Auto scaling custom metric

Scaling policies
      - target tracking scaling ( avg cpu at around 40%)
      - Simple/step scaling (clouldwatch alarm1 - if cpu > 70% add 2 ) (cloudwatch alarm2 - if <30 remove 1 )
      - scheduled actions ( based on usage patterns )

ASG Scaling cooldowns - default is 300 sec - Scaling specific cooldown period to 180 sec if needed
      - ASG does not launch or terminate instances before previous scaling activity takes place
      - Can apply cooldowns to specific simple scaling policy

===============================================================================================================

AWS - EBS Volumes

EBS -  ELASTIC Block storage - Network drive we can attack to instances while they run
    - EC2 loses its root volume when it is terminated
    - Persist data

EBS Volume
      - Network drive ( not a physical drive )
            uses network to communicate to instance and hence latency.
            can be detached and attached to different instances
      - Its locked to an AZ
          can be moved across by creating a snapshot
      - have a provisioned capacity ( GBs and IOPS)
          Billed for provisioned capacity
      - attached to only one instance at a time


cmds -> lsblk
      -> sudo file -s /dev/xvdb
      -> sudo mkfs -t ext4 /dev/xvdb
      -> sudo mkdir /data
      -> sudo mount /dev/xvdb /data

EBS volume Tpes:
      - GP2 (SDD) general purpose balances price and performance - cheap
              - most recommended and used for system boot volumes.
              - virtual desktops
              - low latency interactive apps
              - dev test env
              - 1gb - 16 tb
              - burst IOPS to 3000 , min 100 IOPS
              - 3 IOPS per GB at 5334GB at max 16000 IOPS

      - IO1 (SSD) high performance - mission critical low lantency or high-throughput workloads - expensive
              - critical PIOPS performance or more than 16000 IOPS
              - large DB like mongo, sql, postgres
              - 4Gb- 16 TB
              -  IOPS is provisioned (PIOPS) - min 100-64000 (nitro instances) else 32000
              - max ratio of provisioned IOPS to volume size is 50:1
              - Size of volume and IOPS are independent ( PIOPS) - provisioned IOPS

      - STI (HDD) - LOW cost HDD - frequently accessed throughput intensive workloads - throughput optimized HDD
              - Streaming workloads requiring consistent fast throughput at low price
              - Big data, data warehouse , log processing
              - Apache , Kafka
              - cannot be boot volume
              - 500 GB - 16 TB, MAX IOPS 500 , max throughput of 500 MiB/s - can burst
      - SCI (HDD) - Lowest cost HDD - less frequently accessed workloads but still  high throughput
              - throughput oriented but less frequently accessed
              - cannot be boot volume
              - 500 GB - 16 TB, MAX IOPS 250 , max throughput of 250 MiB/s - can burst

      - characterized by size| throughput | IPOS
      - ONLY GP2 and IO1 are used as boot volumes


Instance Store
        - some instance no root EBS volumes but come with 'instance store '( ephemeral storage)
        - Instance store is physically attached to machine ( EBS network drive )
        Pros:
            - Better I/O performance (HIGH IOPS)
            - good for buffer/cache/temp content
            - data survives reboots
        Cons:
            - on stop/termination instance store is lost
            - no resizing of instance store (no resize)
            - backups must be operated by user

        - disks upto 7.5 TB , stripped to reach 30 TB (can change)
        - risk of data loss if hardware fails

EFS -> Elastic FIle system
        - managed NFS(network file system)  - can be mounted on many EC2
        - EFS works with multi - AZ
        - highly available, scalable , expensive (3 * gp2), pay per use
        - uses NFSv4.1 protocol
        - security group to control access to EFS- add inbound rules to accept the security group
        - compatible with linux based AMI (NOT windows)
        - ex. content management, web serving, data sharing, wordpress
        - POSIX file system - only for linux systems
        - file system scales automatically , pay-per use , no capacity planning


EFS -> Performance and storage classes
      - EFS Scale
            1000s of concurrent NFS clients , 10GB+s throughput
            grow to petabyte-scale
      - Performance mode ( set at EFS creation time )
            - general purpose (default) - lantency sensitive use cases ( web server, CMS)
            - MAX I/O - higher latency , throughput, highly parallel  (big data, media processing)
      - Storage Tiers ( lifecycle management feature - move file after N days)
          - standard : frequently accessed files
          - Infrequent access (EFS-IA) : cost to retrieve files, lower price to store

amazon-efs-utils package

migrate steps ->   EBS -> snapshot -> restore to diff AZ
========================================================================================================================
AWS RDS

RDS -> Relational data service - managed DB service
    - SQL - query language
    - Postgress, mySQL, mariaDB, Oracle, Microsoft SQL server, AURORA ( AWS proprietary DB)

RDS vs DB on EC2
      - Automated provisioning, OS patching
      - continuous backup and restore to specific creationTimestamp
      - monitoring dashboard
      - read replica for improved performance
      - MULTI AZ setup for disaster recovery
      - maintanance windows for upgrades
      - scaling capability (vertical and horizontal)
      - storage backed by EBS (gp2 and IO1)

      -> Cannot SSH into instances

RDS Backups -> Automatically enabled
    - daily full backup of db
    - transaction logs backed up every 5 mins
    - 7 day retention period ( upto 35 days )

    - DB snapshots -> manually triggered -> retention as long as user wants

RDS read replica for read scalability -> Scaling our reads
    -> upto 5 read replicas
    - within AZ, cross AZ, cross Region
    - Replication is ASYNC so reads are eventually consistent
    - replicas can be promoted to their own DB
    - app must update conn string to leverage read replica
    - only SELECT (not UPDATE,INSERT, DELETE )
    - Network cost when data goes from one AZ to another AZ ($$$ cost)

RDS MULTI AZ
      - SYNC replication
      - one DNS name - automatic app failover to standby
      - increase availability- failover possible
      - no manual intervention in apps
      - not used for scaling

Note : We can have read replica's in MULTI AZ for disaster recovery

RDS Security - Encryption
    -> at rest encryption
              - ( can encrypt master and read replicas with aws KMS- AES encryption)
              - if master is not encrypted the read replicas cannot be encrypted
              - TDE - transparent data Encryption available for oracle and SQL server
    -> In-flight Encryption
            - SSL certificates to encrypt data to RDS in flight
            - provide SSL options with trust certificate when conencting to DB
            - to enforce SSL - postgress -> thrugh AWS
                            - mySQL -> within DB (send a command GRANT ...REQUIRE SSL)
    -> Operations - Encypting RDS backups
              - snapshots of un-encrypted RDS db is unencrypted
              - snapshots of encrypted RDS db is encrypted
              - snapshot copy from  unencrypted to encrypted
                   - create snapshot of unencrypted db - copy snapshot and enable encryption - restore db from encrypted snapshot - migrate

RDS Security - Network and IAM
      - usually with private subnet not a public
      - leverage security groups

      - IAM Policies
      - IAM based authentication for RDS mySQL and PostgresSQL only (uses authentication token - lifetime of 15 mins - obtained by RDS API call)
      - Allow only SSL connections

AURORA (not open sourced )  -> AWS DB  ( compatible with mysql and PostgresSQL)
    - Cloud optimized - 5x performance over mysql and 3x over postgress
    - storage automatically grows from 10GB to 64TB
    - can have 15 replicas and faster replication
    - failover is instantaneous . HA native
    - aurora costs 20% more than RDS - but more efficient
    - 6 copies of data in 3 AZ
            - 4 copies of 6 writes
            - 3 copies of 6 for reads
            - self healing with peer - peer replication
            - storage stripped across 100s of volumes
    - One aurora instance takes writes (master )
    - master + upto 15 Aurora read replicas serves reads - supports cross region replication

Aurora DB cluster
      - writer endpoint - pointing to master ( DNS Name)
      - reader endpoint - connection load balancing ( happens at connection level)

Aurora Serverless
      - automated db instantiation and auto-scaling based on actual usage
      - good for infrequent , intermittent or unpredicatable workloads
      - no capacity planning
      - pay per second - cost-effective

GLobal Aurora
    - AURORA cross region read replicas
    - Aurora global database ( recommended)
            - 1 primary region (read/write) - 1 writer multiple readers
            - upto 5 secondary (read only)
            - upto 16 read replicas per secondary region
            - helps decreasing latency
            - RTO recovery time objective < 1min (disaster recovery)

ElasticsCache
      - Get manged redis or Memcached
      - Caches are in-memory DB with really high performance, low latency
      - helps to make apps stateless, reduce load of read intensive workloads
      - write scaling using shards
      - read scaling with replicas, and multi AZ with failover capability
      - aws takes care of OS-maintanance patching etc
      - Use cases - DB cache , User Session store ,

Redis vs Memcached
        - Redis
            - MUTLTI AZ with auto- failover
            - read replicas to scale reads and have HA
            - dada durability using AOF persistance  ( cached data can be stored )
            - backup and restore features
        - Memcached
            - multi-node for patitioning of data (sharding)
            - non persistent
            - no backup and restore
            - multi-threaded architecture

Caching strategies
    - is it safe to cache data?
    - is caching effective for that data?
    - is data structured well for caching ?

Lazy loading / Cache Aside / Lazy population ( all are same ) -CONS-  read penalty ( 3 calls) - stale
    - request record
          - cache hit return record
          - cache miss - request db - return record - app writes to cache

Write through  - Add or update cache when database is updated  -CONS - write penalty ( 2 calls) - cache churn
    - request data
        - cache hit return data
    - write data
        - write to db - write cache
    - this has longer writes, but the reads are quick and the data is always updated in the cache

cache evictions -
    - 3 ways
        - delete item explicity
        - LRU
        - set TTL (time to live)

Multi AZ keeps the same connection string regardless of which database is up.
Read Replicas imply we need to reference them individually in our application as each read replica will have its own DNS name
===============================================================================================================
AWS Route 53

-> Managed DNS (domain name system)

-> Most common records
    - A : hostname to IPv4
    - AAAA - hostanme to IPv6
    - CNAME - hostname to hostname
    - Alias: hostanme to aws resource

    - public domainname
    - private domainname

    - load balancing ( through DNS also called client load balancing)
    - health checks
    - routing policy (simple, failover ,geolocation, latency, weighted, multi value)

    - $0.5 per month per hosted zone

    - global selction - no region specific

route 53 setup
      - buy domain
      - ec2 setup
      - TTL (time to live - HIGH TTL ( less traffic on dns, possibly outdated ) - Low TTL - mandatory field

CNAME vs Alias
    - CNAME : hostname to hostname (app.mydomain.com => blablabla.anything.com)
         ONly for NON ROOT DOMAIN ( something.mydomain.com)
    - ALIAS : Hostname to aws resource ( app.mydomain.com => blablah.amazon.com)
          - Works for both ROOT DOMAIN and NON ROOT domain
          - free of charge
          - native health check
Health checks
    - have x health checks failed => unhealthy (default 3)
    - after X health checks passed => health (default 3)
    - default health check intervals : 30s (can set to 10s - higher cost)
    - about 15 health checkers will check the endpoint health => one request every 2 secs on avg
    - can be http , tcp and https health checks (no SSL verification)
    - possibility of integrating the health check with cloudwatch
    - can be integrated to route 53 DNS


Routing Policy
      - Simple
          - no health checks
          - need to redirect to single resource
          - if multiple values are returned , a random is choosen by client
      - Weighted
          - control the % of requests that go to specific endpoint
          - helpful to split traffic between two regions
          - can be associated with health checks
      - latency
          - redirect to the server that has the least latency close to us
          - super helpful when latency of users is a priority
          - latency is evaluated in terms of user to designated aws region
      - failover
      - Geolocation
          - Diff from latency based. Based on user location
          - traffic from uk to specific IP - default policy for no location specified
      - Multi-Value
          - routing traffic to multiple resources
          - want to associate a route 53 health checks with records
          - upto 8 healthy records are returned for multivalue query
          - multivalue is not substitute for having an ELB

======================================================================================================

AWS VPC

 1- 3 questions
Virtual private cloud
    -  VPC , Subnets, Internet gateways and NAT gateways
    - Security groups, Network ACL( NACL), VPC Flow logs
    - VPC peering, VPC Endpoints
    - site to site VPN & Direct Connect


VPC & Subnets Primer
      - Private network to deploy your resources (regional resource)
      - Subnets allow you to partition your network inside you VPC ( availability zone resource)
      - Public subnet is a subnet that is accessible from the internet
      - private subnet is a subnet that is not accessible from the internet
      - to define access to the internet and between subnets , we use route tables

Internet Gateway & NAT Gateways
      - Internet gateways help our VPC instances connect with the internet (VPC LEVEL)
      - Public subnets have route to the internet gateway

      - NAT gateways ( AWS- Managed) & NAT instances ( self-managed ) allow your
        instances in your private subnet to access the internet while remaining private

Network ACL & Security Groups
    - NACL - subnet level
          - A firewall which controls traffic from and to subnet
          - Can have ALLOW and DENY rules
          - Are attached at subnet Level
          - rules only include IP addresses
          - stateless - return traffic must be explicitly allowed by rules
    - Security Groups - Instance level
          - A firewall that controls traffic to and from an ENI/ an EC2 instance
          - can have only ALLOW rules
          - Rules include IP addresses and other Security Groups
          - stateful  - return traffic is automatically allowed, regardless of any rules

VPC FLOW logs
    - Capture info about IP traffic going into your interfaces
          - vpc flow logs
          - subnet flow logs
          - elastic network interface flow logs
    - can go to S3/ cloudwatch logs

VPC Peering
      - connect two VPC privately using AWS network ( region to region )
      - Must not have overlapping CIDR ( ip address range )
      - VPC peering connection is not transitive ( must be established for each VPC that need to communicate with each other)

VPC Endpoints
      - Endpoints allow you to connect to AWS service using private network instead of the public www network
      - Enhanced security and lower latency to access AWS services
      - VPC Endpoints gateway : S3 & Dynamo DB only
      - VPC Endpoints interface : the Rest of services
      - Only used within your VPC

Site-to-site VPN (premise -> public www -> site to site vpn -> public www -> VPC)
      - connect an on-premise VPN to AWS
      - the connection is automatically encrypted
      - Goes over the public internet

Direct Connect (DX)
      - establish a physical connection between on-premise and AWS
      - the connection is private, secure and fast
      - Goes over private network
      - Takes at least a month to establish

Note: Site-to-ste VPN and DIrect connect cannot access VPC Endpoints

LAMP Stack on EC2
  - Linux os for EC2 Instances
  - Apache : Web server that run on linux (ec2)
  - MySQL- database on RDS
  - PHP: application logic (on EC2)

  - Can add redis/memcached( Elastic Cache) to include a caching tech
  - To store local app data and software: EBS drive (root

===============================================================================================================

AWS S3

- Amazon s3 is one of the main building blocks of AWS
- Infinitely scaling
- many websites use s3
- S3 is global level

S3 - Buckets - Region level
  - Allows people to store objects (files) in "buckets" (directory)
  - Must be globally unique name
  - Defined at region level
  - Naming convention
      - No uppercase
      - No underscore
      - 3-63 chars long
      - Not an IP
      - Must start with lowercase letter or number

S3 - Objects
    - Obects( file) have a key
    - The key is the full path (prefix + object name )
    - There is no concept of directories within buckets
    - object values are content of body
          - Max object size is 5TB
          - if uploading more than 5GB, use "multi part" upload

    - metadata ( list of text key/value pairs - system or user metadata)
    - Tags ( unicode Key/Valye pair - up to 10) - useful for security/lifecyle
    - version ID ( if versioning enable)

presigned url - accessing the bucket object

S3 versioning
      - version you files
      - Enabled at bucket level
      - same key will overwrite will increment the version
      - it is best practice to version you buckets
            - restore
            - roll back
      - Notes : any file not versioned prior to enabling versioning will have version 'null'
                suspending versioning does not delete the previous versions

S3 Encryption
    4 methods for encrypting objects in s3
      - SSE-S3 - encrypts s3 objects using keys handled and managed by AWS
      - SSE-KMS - leverage aws key management service to manage encryption keys
      - SSE-C - When you want to manage you own encryption keys
      - Client side encryption

SSE-S3
    - object encrypted server side
    - AES-256 algo
    - must set header : "x-amz-server-side-encryption": "AES256"

SSE-KMS  - KMS Customer master key (CMK)
    - user control + audit trail (advantages)
    - Object is encrypted server side
    - must set header : "x-amz-server-side-encryption": "aws:kms"

SSE-C - (done through AWS CLI )
    - server side encryption using data keys fully managed by customer outside of AWS
    - Amazon S3 does not store the encryption key you provide
    - https must be used
    - Encryption key must provided in http headers for every http request made

Client-Side
    - client library such as amazon s3 Encryption client
    - must encrypt/decrypt before sending/receiving to/from S3
    - Customer fully manages the keys and encryption cycle

Encrpyption in transit (SSL/TLS)
    - Amazon S3 exposes
          - hTTP endpoint: non encrypted
          - HTTPS endpoint: encryption in flight
    - HTTPS is mandatory for SSE-C
    - Encrpyption in flight is also called SSL/TLS

S3 security
      - user based
            - IAM policies - which API calls should be allowed for a specific user from IAM console
      - Resource based
            - bucket policies - bucket wide rules from s3 console- allows cross account
            - Object Access Control List(ACL) - finger grain
            - Bucket access control list(ACL) - less Common
      - Note: An IAM principal can access an S3 object if
                - User IAM permission allow it OR the resource policy ALLOWS it
                - AND there's no explicit DENY

S3 BUcket policy
    - JSON based policies
          - resources : buckets and objects
          - actions : set of API to allow or deny
          - Effect: ALLOW or Deny
          - principal- the account or user to apply the policy to
    - Use S3 bucket policy to
          - grant public access to the bucket
          - force objects to be encrypted at upload
          - grant access to another account(cross account)

Bucket settings for block public access
    - Block public access to buckets and objects granted through
      - New ACLs
      - any ACLs
      - new public bucket or access point policies

    - Block public and cross-account access to buckets and objects through any public bucket or access point policy
    - can be set at account level

    Networking
      - supports VPC endpoints
    Loggin and Audit
      - S3 access logs can be stored in other s3 bucket
      - API calls can be logged in AWS cloudTrail
    UserSecurity
      - MFA Delete : Multi factor authentication can be Required in versioned buckets to delete objects
      - Pre-signed URLs: URL are valid for limited time

S3 Websites
      - Can host static websites
      - 403 forbidden error - make sure bucket policy allows

S3 Cors (cross origin resource sharing)
      - An origin is a scheme (protocol), host(domain) and port. Ex https://www.example.com ( port is 443 for https, 80 for http)
      - Web browser based mechanism to allow requests to other origins while visiting the main origin
      - same origin : example.com/app1 and example.com/app2
      - diff origin : example.com and other.example.com
      - the request wont be fulfilled unless other origin allows for the requests using CORS header (ACCESS-Control-ALLOW-Origin)
      - Enable correct CORS headers for cross-origin request

S3 - Consistency model
      - read after write Consistency for PUTS of NEW objects
            - PUT 200 OK - GET 200 OK ( as soon as new object is written we can retrieve it)
            - Get 404 -> PUT 200 -> GET 200 (eventually consistent)
     - eventually consistency for DELETES and PUTs of EXISTING objects
            - PUT 200 -> PUT 200 -> GET 200 (might get older version)
            - DELETE 200 -> GET 200
     - Note : There's no way to request "strong consistency"

=============================================================================================================================
     